{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Training Results\n",
    "\n",
    "This notebook loads and analyzes the training results from your GCP workflow run.\n",
    "\n",
    "**Prerequisites:** You should have:\n",
    "1. Completed training on a GCE VM\n",
    "2. Downloaded results from GCS to `./results/<RUN_ID>/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Metrics\n",
    "\n",
    "First, let's find your training run and load the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find the most recent run (or specify RUN_ID manually)\n",
    "results_dir = \"./results\"\n",
    "runs = sorted(os.listdir(results_dir))\n",
    "run_id = runs[-1]  # Most recent run\n",
    "run_path = os.path.join(results_dir, run_id)\n",
    "print(f\"Analyzing run: {run_id}\")\n",
    "print(f\"Run path: {run_path}\")\n",
    "\n",
    "# Load metrics\n",
    "with open(os.path.join(run_path, \"metrics.json\")) as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(f\"\\nTraining completed {len(metrics['epoch'])} epochs\")\n",
    "print(f\"Final accuracy: {metrics['test_accuracy'][-1]:.2f}%\")\n",
    "print(f\"Final loss: {metrics['test_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot Training Curves\n",
    "\n",
    "Visualize how loss and accuracy evolved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(metrics['epoch'], metrics['test_loss'], 'b-o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Test Loss over Training')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(metrics['epoch'], metrics['test_accuracy'], 'g-o', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Test Accuracy over Training')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(run_path, 'training_curves.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to: {os.path.join(run_path, 'training_curves.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect the Model\n",
    "\n",
    "Load the saved model and examine its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = os.path.join(run_path, 'model.pt')\n",
    "model_state = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "print(\"Model layers:\")\n",
    "for name in model_state.keys():\n",
    "    shape = model_state[name].shape\n",
    "    print(f\"  {name}: {list(shape)}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model_state.values())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Display a summary of your training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TRAINING RUN SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Run ID:          {run_id}\")\n",
    "print(f\"Epochs:          {len(metrics['epoch'])}\")\n",
    "print(f\"Final Accuracy:  {metrics['test_accuracy'][-1]:.2f}%\")\n",
    "print(f\"Final Loss:      {metrics['test_loss'][-1]:.4f}\")\n",
    "print(f\"Best Accuracy:   {max(metrics['test_accuracy']):.2f}% (epoch {metrics['test_accuracy'].index(max(metrics['test_accuracy'])) + 1})\")\n",
    "print(f\"Parameters:      {total_params:,}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nFiles in {run_path}:\")\n",
    "for f in os.listdir(run_path):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
